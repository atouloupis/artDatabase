[
{"article": "            What is this article about? When I first learned about ImmutableJS I was convinced it was a great addition to any project. So I tried to make it a working standard at Theodo. As we offer scrum web development teams for various projects going from POC for startups to years lasting ones for banks, we bootstrap applications all the time and we need to make a success of all of them! So we put a strong emphasis on generalizing every finding and learning made on individual projects. With this objective in mind we are determining which technical stack would be the best for our new projects. Each developer contributes to this effort by improving the company\u2019s technical stack whenever they makes a new learning. When React was chosen as our component library we were just embarking on a hard journey that you may have experienced: making the dozens other decisions that come with a React app. React then what? One of those choices lead us to choose redux as our global state management lib. We noticed that people were having troubles with Immutability in reducers, one of the three core principles of Redux The first possibility to answer this purpose is defensive copying with ES6 spread operator { ...object }. The second option we studied is Facebook\u2019s ImmutableJS library.  Using ImmutableJS in your react-redux app means that you will no longer use JS data structures (objects, arrays) but immutable data structures (i.e. Map, List, \u2026) and their methods like .set(), .get(). Using .set() on a Map, the Immutable equivalent of objects, returns a new Map with said modifications and does not alter previous Map. In order to make an informed choice I observed the practices on projects that used either one of the two, gathered their issues and tried to find solutions for them. This article is the result of this study and hopefully it will help you choose between those two! In this first part I will compare those two options in the light of 3 criteria out of the 5 I studied: readability, maintainability and learning curve. The second part of this study, coming soon, will explore performance and synergy with typing tools. First Criterion, Readability: Immutable Wins! If your state is nested and you use spread operators to achieve immutability, it can quickly become unreadable: function reducer(state = defaultState, action) {    switch (action.type) {      case 'SET_HEROES_TO_GROUP':        return {          ...state,          guilds: {            ...state.guilds,            [action.payload.guildId]: {              ...state.guilds[action.payload.guildId],              groups: {                ...state.guilds[action.payload.guildId].groups,                [action.payload.groupId]: {                  ...state.guilds[action.payload.guildId].groups[action.payload.groupId],                  action.payload.heroes,                },              },            },          },        };    }  }   If you ever come across such a reducer during a Code Review there is no doubt you will have troubles to make sure that no subpart of the state is mutated by mistake. Whereas the same function can be written with ImmutableJS in a much simpler way function reducer(state = defaultState, action) {    switch (action.type) {      case 'SET_HEROES_TO_GROUP':          return state.mergeDeep(          state,          { guilds: { groups: { heroes: action.payload.heroes } } },        ).toJS();    }  }   Conclusion for Readability ImmutableJS obviously wins this criteria by far if your state is nested. One counter measure you can take is to normalize your state, for example with normalizr. With normalizr, you never have to change your state on more than two levels of depth as shown on below reducer case. // Defensive copying with spread operator  case COMMENT_ACTION_TYPES.ADD_COMMENT: {    return {      ...state,      entities: { ...state.entities, ...action.payload.comment },    };  }    // ImmutableJS  case COMMENT_ACTION_TYPES.ADD_COMMENT: {    return state.set('entities', state.get('entities').merge(Immutable.fromJS(action.payload.comment)));  }   Second Criterion, Maintainability and Preventing Bugs: Immutable Wins Again! A question I already started to answer earlier is: Why must our state be immutable?  Because redux is based on it Because it will avoid bugs in your app  If for example your state is: const state = {    guilds: [      // list of guilds with name and heroes      { id: 1, name: 'Guild 1', heroes: [/*array of heroes objects*/]},    ],  };   And your reducer case to change the name of a guild is: switch (action.type) {    case CHANGE_GUILD_NAME: {      const guildIndex = state.guilds.findIndex(guild => guild.id === action.payload.guildId);        const modifiedGuild = state.guilds[guildIndex];      // here we do a bad thing: we modifi the old Guild 1 object without copying first, its the same reference      modifiedGuild.name = action.payload.newName;        // Here we do the right thing: we copy the array so that we do not mutate previous guilds      const copiedAndModifiedGuilds = [...state.guilds];      copiedAndModifiedGuilds[guildIndex] = modifiedGuild;        return {        ...state,        guilds: copiedAndModifiedGuilds,      };    }  }   After doing this update, if you are on a detail page for Guild 1, the name will not update itself! The reason for this is that in order to know when to re-render a component, React does a shallow comparison, i.e. oldGuild1Object === newGuild1Object but this only compares the reference of those two objects. We saw that the references are the same hence no component update. An ImmutableJS data structure always returns a new reference when you modify an object so you never have to worry about immutability. Using spread operators and missing one level of copy will make you waste hours looking for it. Another important issue is that having both javascript and Immutable objects is not easily maintainable and very bug-prone. As you cannot avoid JS objects, you end up with a mess of toJS and fromJS conversions, which can lead to component rendering too often. When you convert an Immutable object to JS with toJS, it creates a new reference even if the object itself has not changed, thus triggering component renders. Conclusion for Maintainability Immutable ensures you cannot have immutability related bugs, so you won\u2019t have to check this when coding or during Code review. One way to achieve the same without Immutable would be to replace the built-in immutability with immutability tests in your reducers. it('should modify state immutably', () => {    const state = reducer(mockConcatFeedContentState, action);      // here we check that all before/after objects are not the same reference -> not.toBe()    expect(state).not.toBe(mockConcatFeedContentState);    expect(state.entities).not.toBe(mockConcatFeedContentState.entities);    expect(state.entities['fakeId']).not.toBe(mockConcatFeedContentState.entities['fakeId']);  });   But making sure that your team-mates understand and always write such tests can be as painful as reading spread operators filled reducers. My opinion is that Immutable is the best choice here on the condition that you use it as widely as possible in your app, thus limiting your use of toJS. Third Criterion, Learning Curve: One point for Spread Operators One important point when assessing the pros and cons of a library/stack is how easy will it be for new developers to learn it and to become autonomous on the project. The results of my analysis on half a dozen projects using is that learning ImmutableJS is hard work. You have a dozen data structures to choose from, about two dozen built-ins or methods that sometimes do not behave the same way javascript methods do. Below are some examples of such differences: const hero = {    id: 1,    name: 'Superman',    abilities: ['Laser', 'Super strength'],  }    const immutableHero = Immutable.fromJS(hero); // converts objects and arrays to the ImmutableJS equivalent      // get a property value  hero.abilities[0] // 'Laser'  immutableHero.get('abilities', 0) // 'Laser'    // set a property value  hero.name = 'Not Superman'  immutableHero.set('name', 'Not Superman')    immutableHero.name = 'Not Superman' // nothing happens!    // Number of elements in an array / Immutable equivalent  hero.abilities.length // 2  hero.get('abilities').size // 2    // Working with indexes  const weaknessIndex = hero.abilities.indexOf('weakness') // -1  hero.abilities[weaknessIndex] // throws Error    const immutableWeaknessIndex = immutableHero.get('abilities').indexOf('weakness') // -1  immutableHero.get('abilities').get(weaknessIndex) // 'Super strength'   While you can use all the knowledge you have on javascript and ES6, if you go with ImmutableJS you\u2019ll have to learn some things from the start. Nicolas, a colleague of mine once came to me with a strange issue. They were using normalizr and had a state that looked like the Immutable equivalent of this: {    fundState: {      fundIds: // a list of fund ids      fundsById: // an object with a fund id as key and the fund data as value: { fund1Id: fund1 },    }  }   Their problem was that their ids, indexing funds in fundsById Map where strings of numbers and not numbers. At least twice, one of their developers had a hard time writing a feature because they were trying to get the funds like this: state.get('fundState', 'fundsById', 3) to get the fund of id 3. The issue here is that contrary to javascript, strings of numbers and numbers are not at all interchangeable (it may be a good thing but it is an important difference!). So they had to convert all their id keys to the right type. Another issue that colleagues shared with me was that ImmutableJS is really hard to debug in the console as shown below with our immutableHero object from above:  As you can see, it\u2019s nearly unreadable and its only a really simple object! A great solution I encountered when trying to help them is immutable formatter a chrome extension that turns what you saw into this beauty:  To enable it, you have to open chrome dev tools. Then access the dev tools settings and check \u201cenable custom formatter\u201d option:  In the case of ES6, new developers have three things to learn:  Understand why immutability is important and why they should bother How to use spread operators to enforce immutability Not to use object.key = value to modify their state  Conclusion for Learning Curve Overall the learning curve for spread operators, an ES6 tool is rather easy since you can still use all the javascript you know and love but you must be careful to the points listed above. ImmutableJS on the other hand will be much harder to learn and master. Conclusion for Part 1 In conclusion, this first part showed us that ImmutableJS comes with a lot of nice things, allows you to concentrate on working on value added work rather than trying to read horrible reducers or looking for hidden bugs. This of course is at the cost of the steeper learning curve of a rich API and some paradigms different from what you are used to! In the part II of this article I will compare both solutions in the light of Performance and compatibility with typing. If you liked this article and want to know more, follow me on twitter so you know when the second part is ready :). @jeremy_dardour             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           J\u00e9r\u00e9my Dardour                                                  "},
{"article": "            When\u00a0deciding what external payment service you want to use, you need to take into account several factors: the price of the payment provider, the amount of time of implementation, the ease of customising and styling the form, the trust of users to the company \u2026 and see what is best suited for your needs. There is not one best online provider, but this article will\u00a0help you chose one that fits you project. At Theodo we use different online payment providers: SagePay, PayPal and Stripe for\u00a0our websites. For each one of them we have discovered advantages and drawbacks, that I will share with you. Keep in mind that using a payment provider such as one we are going to talk about allows to easily be PCI\u00a0compliant, which is\u00a0essential for any website dealing with card data. Overall price The overall price covers the development cost and the\u00a0transaction fees. Additional costs for setup, refunding and breaking the contract may also apply. Development cost: Here is\u00a0in order a comparison of the three payment providers\u00a0we use:  1 \u2013 Stripe: ~1day. This \u201cdevelopment first\u201d payment provider is specially built for an easy integration to websites. Therefore it is no surprise that\u00a0it is a good choice if you want to quickly\u00a0handle payments. In our company we like to use it\u00a0when building MVPs because it takes less than a day to integrate and design with\u00a0Stripe Elements. 2 -PayPal express checkout: ~ 2/3 days. You can usePayPal Express Checkout on your website to allow your customers to proceed to aPayPal payment. This will momentarily redirect the client to the PayPal login page and then a summary page where he can pay he will then be sent back to your website. Integrating Paypal takes a couple of days. 3 \u2013 SagePay: ~ 1 week. Out of the three payment providers we use, this is definitely the one that takes the longest to integrate \u2013 all in all more than a week. You can use an iFrame to send the card data. However the documentation is not that clear and styling the form is complex (you need to send the styling files to SagePay that will then add them to the iFrame).  Fees per transaction: The price depends on:  Number of transaction per month Price per transaction you will charge Debit or Credit card \u2026  From\u00a0our experience we found that\u00a0for websites selling lots of products\u00a0at\u00a0small prices (~10\u20ac) it is worth using SagePay. But if\u00a0there is\u00a0a smaller traffic and higher prices Stripe might be a better solution. In both casesPayPal tends to have higher fees. Finally, companies often negotiate the price fees directly with the payment providers to get more interesting offers, but this can\u00a0take some time. Here is an example of what you would be paying\u00a0to the different companies: If your company sells 100 products a month at an average price of \u00a340 (total of \u00a34,000), these would be the prices:  Stripe: \u00a381 SagePay: \u00a3103.5 PayPal: \u00a3136  But if your company sells 350 products a month at an average price of \u00a310 (total of \u00a33,500), these would be the prices:  Stripe: \u00a3136.5 SagePay: \u00a393.05 PayPal: \u00a3171  Here are the fees that you can find on the 3 websites:  Stripe:  For VISA Mastercard and American Express:  1.4% + 25p / transaction for European cards 2.9% + 25p / transaction for non European cards   As they say on their website: \u2019No setup, monthly, or hidden fees\u2019. Over \u00a320,000 per months you can negotiate for lower fees https://stripe.com/gb/pricing   SagePay:  \u00a319.90/month: 350 free transactions per month then 12p per transaction after \u00a345/month if max 500 free token purchases per month then 10p per transaction If more than 3000 transactions per month you will need to contact Sagepey to get a corporate account + 2.09% for Mastercard or Visa credit cards + 40p for debit cards (fees a quite hidden) Cancelation fees can be high, a minimum of 3 months notice is necessary. https://www.sagepay.co.uk/our-payment-solutions/online-payments   PayPal:  Less than \u00a31500/month:\u00a03.4%\u00a0+ 20p per transaction Less than \u00a36000/month:\u00a02.9%\u00a0+ 20p per transaction Less than \u00a315,000/month:\u00a02.4%\u00a0+ 20p per transaction Less than \u00a355,000/month:\u00a01.9%\u00a0+ 20p per transaction More: personalised amount https://www.paypal.com/uk/webapps/mpp/paypal-fees    Website Integration / design The design of the form is very important. Users probably will not trust a website with cheap design. Also, paying is not the most pleasant moment of a customer\u2019s journey. A seamless flow\u00a0should be a must-have\u00a0to\u00a0get customers to pay and come back. Do not underestimate the design of your form!  Stripe: you can easily style the different inputs so the payment is consistent with the rest of your website. This is something we really appreciate with Stripe Elements. PayPal: as the payment is done directly onPayPal website you won\u2019t have any design to do! Users would find this option reassuring because they know how there money is being processed. SagePay: it is difficult to get a flawless and consistent design, as you have to send files to SagePay so they can handle the iFrame styling.  What we recommend If you wish to add a payment method to your website for the first time and that the project is short, we would recommend Stripe. As it is really easy to integrate and style it is perfect for these projects. If your website has a lot of traffic, SagePay is a good choice because of its low fees with a lot of transactions. However keep in mind that the implementation can take time.\u00a0Finally it is a nice option to add aPayPal button on top of your existing payment methods as some customers are reluctant to input the card details on websites.             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Alice Breton                                                  "},
{"article": "            AppCenter is a great CI platform for Mobile Apps. At Theodo we use it as the standard tool for our react-native projects. In a recent project, we needed to have a shared NPM package between the React and React-Native applications. There is no mention of how to achieve this in the AppCenter documentation, and if you ask their support they will say it\u2019s not possible. Me: Hello, we\u2019re wanting to have a shared library used in our project. This would require an npm install from a private NPM repo (through package cloud). What is the best practice for adding a private npm access on the AppCenter CI? Microsoft: We currently only support cloud git repositories hosted on VSTS, Bitbucket and GitHub. Support for private repos is not available yet but we are building new features all the time, you can keep an eye out on our roadmap for upcoming features. Luckily there is a way!  AppCenter provides the ability to add an `appcenter-post-clone.sh` script to run after the project is cloned. To add one, just add a file named `appcenter-post-clone.sh`, push your branch and, on the configure build screen, see it listed.  Pro Tip: You need to press \u201cSave and Build\u201d on the build configuration after pushing a new post-clone script on an existing branch. Now, what to put in the script? Having a .npmrc in the directory you run \u2018npm install\u2019 or \u2018yarn install\u2019 from allows you to pass authentication tokens for new registries. We want a .npmrc like this:   always-auth=true  registry=https://YOUR_PACKAGE/NAME/:_authToken=XXXXXXXX   Obviously, we don\u2019t really want to commit our read token to our source code, therefore we should use an environment variable. So we can add to our post-clone script:   touch .npmrc  echo \"always-auth=true  registry=https://YOUR_PACKAGE/NAME/:_authToken=${READ_TOKEN}\" > .npmrc   Now, on AppCenter, we can go into the build configuration and add an environment variable called \u2018READ_TOKEN\u2019.  Now rebuild your branch and your package installs should pass.             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Ben Ellerby               I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed.     I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.    https://www.linkedin.com/in/benjaminellerby/                                   "},
{"article": "            Thanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable.  The principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique. Basically, here is an example of my git history before and after I used this technique.  Stay focus, rebase and merge are no joke!  What is the goal of a rebase or a merge ? Rebase and merge both aim at integrating changes that happened on another branch into your branch. What happens during a merge ? First of all there are two types of merge:  Fast-forward merge 3-way merge  Fast-forward merge A fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging. The following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software. A: the branch in which you are merging B: the branch from which you get the modifications  git checkout A git merge B   As you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same. Notes:  git checkout A, git rebase B you would have had the exact same result! git checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.  3-way merge A 3-way merge happens when both branches have had new commits since the last shared commit. The following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software. A: the branch in which you are merging B: the branch from which you get the modifications  git checkout A git merge B   During a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:  All the modifications brought by the three commits from B (in purple) The possible conflict resolutions  Git will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge. The default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge. What happens during a rebase? A rebase differ from a merge in the way in which it integrates the modifications. The following drawings show what happens during a rebase and how it is shown in a graphical git software. A: the branch that you are rebasing B: the branch from which you get the new commits  git checkout A git rebase B    When you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one. For each commit to apply, if there are conflicts, they will be resolved inside of the commit. After a rebase, the new commits from A (in blue) are not exactly the same as they were:  If there were conflicts, those conflicts are integrated in each commit They have a new hash  But they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple. What is the best solution to integrate a new feature into a shared branch and keep your git tree clean? Let say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it. You have two main solutions:  First solution:   git checkout feature git rebase master git checkout master git merge feature   Note : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only Second solution:  git checkout master git merge feature   As you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened. In this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch! Unfortunately, the first solution has a few drawbacks: History rewriting When you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place. This can potentially erase changes another developer made, or introduce conflicts resolution for him. To avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch. However you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve. The obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all. Conflicts resolution When you merge or rebase, you might have to resolve conflicts. What I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch. The only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one. Conclusion To conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:  \u00a0             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us               WRITTEN BY                                                           J\u00e9r\u00e9mie Marniquet Fabre               I am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !                                   "},
{"article": "            Through my experiences, I encountered many fellow coworkers that found CSS code painful to write, edit and maintain. For some people, writing CSS is a chore. One of the reasons for that may be that they have never been properly taught how to write good CSS in the first place, nor what is good CSS. Thus it has an impact on their efficiency and on the code quality, which isn\u2019t what we want. This two parts article will focus on:  Part 1: What is good CSS code? (more precisely, what is not good CSS). I will focus on actionable tips and tricks to avoid creating technical debt starting now. Part 2: how to migrate from a complex legacy stylesheet to a clean one.  Warning: these are the guidelines that I gathered through my experiences and that worked well for many projects I worked on. In the end, adopt the methods that fit your needs. Requirements I assume that you are looking for advice to improve yourself at writing CSS, thus you have a basic knowledge of CSS and how it works. In addition, you will need these things: A definition of done You should be very clear about which browser/devices you want to support or not. You must list browsers/devices you want to support and stick to it. Here is an example of what can be a definition of done:  Browsers: Chrome \u2265 63, Firefox \u2265 57, Edge \u2265 12 Devices: laptops with resolution \u2265 1366*768  You must write this list with a business vision: maybe your business needs IE support because 20% of your users are using it. You can be specific for some features. For instance: the landing page should work on devices with small screens but the app behind the login should not. But if your Definition Of Done does not include IE9, do not spend unnecessary time fixing exotic IE9 bugs. From there you can use caniuse.com to see which CSS features are supported on your target browsers (example below).  A good understanding of what specificity is Here is a quick reminder about what is specificity: Specificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins. The rules to win the specificity wars are:  Inline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover,\u00a0::before\u2026). Classes win over element selectors. A more specific selector beats any number of less specific selectors. For instance,\u00a0.list\u00a0is more specific than\u00a0div ul li. Increasing the number of selectors will result in higher specificity. .list.link\u00a0is more specific than\u00a0.list and .link. If two selectors have the same specificity, the last rule read by the browser wins. Although !important\u00a0has nothing to do with the specificity of a selector, it is good to know that a declaration using !important overrides any normal declaration. When two conflicting declarations have the !important keyword, the declaration with a greater specificity wins.  Here is a good website to compute the specificity of a selector:\u00a0Specificity Calculator. Below is a chart to recap all these rules (taken from this funny post on specificity).  Some basic knowledge of preprocessors Preprocessors are great because they allow you to write CSS faster. They also help to make the code more understandable and customizable by using variables. Here I will use a SCSS syntax in the examples (my favorite preprocessor but others like LESS/Stylus are pretty similar). An example of what you can do with preprocessors: // vars.scss  $messageColor: #333;    // message.scss  @import 'vars';  %message-shared {      border: 1px solid black;      padding: 10px;      color: $messageColor;  }    .message {      @extend %message-shared;  }  .success {      @extend %message-shared;      border-color: green;  }   Variables in CSS can now be done with native CSS but preprocessors still have the upper hand on readability/usability. What you should not do I will show you what you DON\u2019T want to do and explain why such coding practices will lead to many problems over time. Don\u2019t write undocumented CSS I put this point first because I believe it\u2019s one of the most impactful things you can act on straightaway. Like any other language, CSS needs to be commented. Most stylesheets don\u2019t have comments. And when I advise you to write comments, I don\u2019t talk about this: // Header style  .header {}   Those are bad comments because they have no purpose and convey no additional information. A good CSS comment explains the intention behind a selector/rule. Here is an example of some good comments: .suggestions {      // 1 should be enough but in fact there is a Bootstrap rule that says      // .btn-group>.btn:hover z-index: 2 (don't ask me why they did this)      z-index: 3;  }    // Firefox doesn't respect some CSS3 specs on the box model rules  // regarding height. This is the only cross-brower way to do an   // overflowing-y child in a fixed height container.  // See https://blogs.msdn.microsoft.com/kurlak/2015/02/20/filling-the-remaining-height-of-a-container-while-handling-overflow-in-css-ie8-firefox-chrome-safari/  .fixed-height-container {}   What should you comment on?  CSS hacks Every line you didn\u2019t write or you wrote 6 months ago (which is the same) where you needed more than 10 seconds to understand its intended purpose. Magic values. Speaking of which\u2026  Don\u2019t use magic values The most common thing between developers resenting CSS is a general feeling of black magic. Put a rule here and an !important there, with a width value that looks good and it works. You see? Magic. But magic doesn\u2019t exist. You should have a more scientific approach to demystify CSS. Writing good comments is one thing. Stopping writing magic values is another. I define a magic value by \u201cany value that looks weird, aka is not a multiple of 5\u201d \u2013 even then some values may look weird. Examples of magic values are: left: 157px;  height: 328px;  z-index: 1501;  font-size: 0.785895rem;   Why are these values problematic? Because again, they do not convey the intention. What is better:  Using preprocessor variables which adds a meaning to a number. Make the exact calculation. If you wrote this value after some tests using the Chrome dev tools you may find out with a scientific approach that your initial \u201cmagic\u201d value may not be the most accurate one. Commenting the value to explain why it\u2019s here. Challenging your value/unit and changing it to a more pertinent one.  Example: left: calc(50% - ($width / 2));  // An item have a 41px height:  // 2*10px from padding+20px from line-height+1px from one border.  // So to get 8 items in height:  height: 8 * 41px;  z-index: 1501; // Needs to be above .navbar  font-size: 0.75rem;   Don\u2019t use px units everywhere Most hellish CSS stylesheets use px units everywhere. In fact, you should almost never use them. In most cases, pixels never is the good unit to use. Why? Because they don\u2019t scale with the font-size or the device resolution. Here is a recap of which unit to use depending on the context. Quick cheat sheet:  px: do not scale. Use for borders and the base font size on the html element. That\u2019s all. em, rem (> IE8): scale with the font-size. 1.5em is 150% of the font size of the current element. 0.75rem is 75% of the font size of the html element. Use rem for typography and everything vertical like margins and paddings. Use em wisely for elements relative to the font-size (icons as a font for instance) and use it for media query breakpoints. %, vh, vw (> IE8): scale with the resolution. vh and vw are percentages of the viewport height and width. These units are perfect for layouts or in a calc to compute the remaining space available (min-height: calc(100vh - #{$appBarHeight})).  I made a page for you to play with the base font-size and CSS units (open in a new window to resize the viewport and try changing the zoom setting). Don\u2019t use !important You should keep your specificity as low as possible. Otherwise, you will be overriding your override rules. If you tend to override your styles, with time passing you will hit the hard ceiling \u2013 !important and inline style. Then it will be a nightmare to create more specific CSS rules. Using !important is a sign that you\u2019re working against yourself. Instead, you should understand why you have to do this. Maybe refactoring the impacted class will help you, or decoupling the common CSS in another class would allow you not to use it and lower your specificity. The only times you should use it is when there is absolutely no other way to be more specific than an external library you are using. Don\u2019t use IDs as selectors Keep. Your. Specificity. Low. Using an ID instead of a class condemn you to not reuse the code you\u2019re writing right now. Furthermore, if your javascript code is using IDs as hooks it could lead to dead code (you are not certain whether you can remove this ID because it could be used by the JS and/or CSS). Instead of using IDs, try to look up common visual patterns you could factorize for future reuse. If you need to be specific, add a class on the lowest level of the DOM tree possible. At the very least, use a class with the name you would have given to your ID. // Don't  #app-navbar {}    // Slighlty better  .app-navbar {}    // Better (pattern that you could reuse)  .horizontal-nav {}   Don\u2019t use HTML elements as selectors Again. Keep your specificity low. Using HTML tags as selectors goes against this because you will have to add higher-specificity selectors to overwrite them later on. For instance, styling the a element (especially the a element, with all its use cases and different states) will be an inconvenience when you use it in other contexts. // Don't  <a>Link</a>  <a class=\"button\">Call to action</a>  <nav class=\"navbar\"><a>Navbar link</a></nav>  a { ... }  .button { ... }  // Because you will have to create more specific selectors  a.button { ...overrides that have nothing to do with the button style... }  .navbar a { ...same... }    // Better  <a class=\"link\">Link</a>  <a class=\"button\">Call to action</a>  <nav class=\"navbar\"><a class=\"navbar-link\">Navbar link</a></nav>  .link { ...style of a link, can be used anywhere... }  .button { ...style of a button, idem... }  .navbar-link { ...style of a navbar link, used only in navbars... }   However, there are some cases when you could use them, for instance when a user wrote something like a blog post that is output in pure HTML format (therefore preventing you from adding custom classes). // Don't  ul { ... }    // Better  %textList { ... }  .list { @extends %textList; }  .user-article {      ul { @extends %textList; }  }   Furthermore, HTML should be semantic and the only hooks for style should be the classes. Don\u2019t be tempted to use an HTML tag because it has some style attached to it. A side-note on the ideal specificity You should aim for a specificity of only one class for your CSS selectors. The best part in Cascading Style Sheets is \u201ccascading\u201d. The worst part in Cascading Style Sheets is \u201ccascading\u201d \u2014 The Internet The whole thing about CSS is that you want to make your style the same everywhere (therefore it needs to be reusable) AND you want to make it different in some unique places (therefore it needs to be specific). All CSS structure issues are variations of this basic contradiction. Opinion: The Cascading effect of CSS can be a great tool and it serves a purpose: to determine which CSS declaration is applied when there is a conflict. But it is not the best tool to solve this problem. What if instead, there were no conflicts on CSS declarations, ever? We wouldn\u2019t need the Cascade effect and everything would be reusable. Even \u201csuper-specific\u201d code can be written as a class that will be used only once. If you use selectors of only one class, you will never need to worry about specificity and overwriting styles between components. \u201cBut that could lead to a lot of duplicated source code\u201d, you could say. And you would be right if there were no CSS preprocessors. With preprocessors, defining mixins to reuse bits of CSS by composition is a great way to factor your code without using more specific selectors. There is still a concern over performance because the output stylesheet is bigger. But for most stylesheets/projects, CSS performance is irrelevant over javascript concerns. Furthermore, the advantage of maintainability far outweighs the performance gains. If we try to combine the last three Don\u2019ts, this is how I would take this code: <main id=\"main_page\">      <p><a>Some link</a></p>      <footer>          <a>Some other link</a>      </footer>  </main>    a {      cursor: pointer;  }    #main_page {      a {          color: blue;            &:hover {              color: black;          }      }  }    footer {      border: 1px solid black;        a {          color: grey !important;      }  }   And turn it into this: <main>      <p><a class=\"link\">Some link</a></p>      <footer class=\"footer\">          <a class=\"footer-link\">Some other link</a>      </footer>  </main>    .link {      cursor: pointer;      color: blue;        &:hover {          color: black;      }  }    .footer {      border: 1px solid black;        &-link {          // You can use a mixin here if there is a need to factor in          // the common code with .link          cursor: pointer;          color: grey;      }  }   What you can do right now Do try to understand how CSS declarations really work There are some declarations you really want to understand. Because if you don\u2019t there will still be a feeling of \u201cblack magic\u201d happening.  vertical-align: middle; margin: 0 auto; all the things! What you should know (tip: if you think you would not be able to explain it clearly to someone else, click the links):  The box model (width, height, padding, margin, border, box-sizing, display: block/inline/inline-block). Positioning and positioning contexts (position: static/relative/absolute/fixed, z-index). Typography (font-size, font-weight, line-height, text-transform, text-align, word-break, text-overflow, vertical-align) Selectors (*, >, +, ::before, ::after, :hover, :focus, :active, :first-child, :last-child, :not(), :nth-child())  Bonus ones to go further:  A complete guide to tables Transitions Shadows & filters Floats (only if you have to. My advice would be: don\u2019t use floats).  Do look at Flexbox and Grid If your Definition of Done doesn\u2019t include older browsers and you don\u2019t use/know the flexbox and/or grid model, it will solve a lot of your layout problems. You may want to check these great tutorials:  A complete guide to Flexbox (Chrome \u2265 21, Firefox \u2265 28, IE \u2265 10, Safari \u2265 6.1) A complete guide to Grid (Chrome \u2265 57, Firefox \u2265 52, IE \u2265 10, Safari \u2265 10.3), a short example of grid use  An example of a possible layout implementation possible with Grid and that is not a nightmare to implement:  Do look at BEM and CSS modules/styled components and apply it to new components You should use CSS guidelines such as BEM. It will make your code more maintainable/reusable and prevent you from going down into the specificity hell. Here is a great article on BEM which I recommend. Furthermore, if you have a component-based architecture (such as React, Vue or Angular), I recommend CSS modules or styled components to remove the naming hassle of BEM (here is an article on the whole topic). Opinion: there is one main gotcha with these tools. You may believe that the auto-scoping feature of these tools acts as a pseudo-magic protection. However, beware that you should not bypass the above Don\u2019ts. For instance, using HTML elements in CSS modules selectors destroys the purpose of auto-scoping because it will cascade to all children components. You should also keep a strict BEM-like approach (structuring your component styles into blocks, elements, and modifiers) while using these kinds of tools. Do challenge and remove useless CSS A lot can be done by using only seven CSS properties. Do challenge CSS that does not seems essential. Is this linear-gradient background color essential when nobody sees the gradient effect? Are those box-shadow declarations really useful? You can also find unused CSS with Chrome\u2019s CSS coverage. In the \u201cMore tools\u201d drop-down, activate the \u201cCoverage\u201d tool, start recording and crawl your target pages. Here is an example showing that the .TextStat class is never used, as well as 70% of the whole stylesheet.  Do it yourself A note on frameworks like Bootstrap and others: they are useful for small and quick projects when you don\u2019t have time to dedicate to style. But for many medium-sized and a lot of large-sized projects, don\u2019t use them. Over time, you will need to overwrite them and it will eventually take more time than doing it yourself because it will produce a more complex and more specific code. In addition, doing your style yourself makes you learn a lot. UI designer is a whole job so creating a UI from scratch is a real challenge. At first, try to reproduce the same look and feel than other websites you like (you can look at the code with the browser dev tools). My personal experience is that I started to love and learn CSS the moment I threw Bootstrap out the window for a personal project and started writing my own style.  I hope that with all the above best practices you will feel more comfortable writing CSS and that it will help you enhance your code quality. In Part 2 I will address the hassle of migrating a hellish complex stylesheet full of black magic to a clean, understandable and maintainable one. So don\u2019t hesitate to share your CSS horror stories!             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Alb\u00e9ric Trancart               Alb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.                                   "},
{"article": "            Usually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering. The problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page! Looking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications. Why SSR (Server-Side Rendering)? This is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:  Improve your SEO Speed up your first page load Avoid blank page flicker before rendering  If you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering. But let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here. What\u2019s the plan? For this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings. This article is split in 3 sections matching 3 server-side-rendering strategies:  How tomanually upgrade your React app to get\u00a0SSR How to start with Next.js from scratch Migrate your existing React app to server-side with Next.js  I won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon). 1) Look how twisted manual SSR is\u2026  In this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:  package.json\u00a0for dependencies Webpack configuration included App.js\u00a0\u2013 loads React and renders the Hello component index.js\u00a0\u2013 puts all together into a root component  Checking rendering type I just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window: const isClientOrServer = () => {    return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';  };   so that we display on the page what is rendering the application: server or client. Test it by yourself  clone\u00a0this repository checkout the initial commit install the dependencies with\u00a0yarn launch the dev server with\u00a0yarn start browse to\u00a0http://localhost:3000\u00a0to view the app  I am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:  Implementing\u00a0Server-side Rendering Let\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps. We first need a node server using Express:\u00a0yarn add express. In our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer. // use port 3001 because 3000 is used to serve our React app build  const PORT = 3001; const path = require('path');    // initialize the application and create the routes  const app = express();  const router = express.Router();    // root (/) should always serve our server rendered page  router.use('^/$', serverRenderer);   To render our html, we use a server renderer that is replacing the root component with the built html: // index.html file created by create-react-app build tool  const filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');    fs.readFile(filePath, 'utf8', (err, htmlData) => {    // render the app as a string    const html = ReactDOMServer.renderToString(<App />);      // inject the rendered app into our html    return res.send(      htmlData.replace(        '<div id=\"root\"></div>',        `<div id=\"root\">${html}</div>`      )    );  }   This is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string. We finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel. require('babel-register')({    ignore: [ /(node_modules)/ ],    presets: ['es2015', 'react-app']  });   Test it by yourself  checkout last changes on master branch install the dependencies with\u00a0yarn build the application with\u00a0yarn build declare babel environment in your terminal:\u00a0export BABEL_ENV=development launch your node server with\u00a0node server/bootstrap.js browse to\u00a0http://localhost:3001\u00a0to view the app  Still simulating the \u20183G network\u2019 in Chrome, here is the result:  Do not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client. We proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?! We\u2019re missing many features Our example is a good proof of concept but very limited. We would like to see more features like:  import images in js files (logo problem) several routes usage or route management (check\u00a0this article) deal with the\u00a0</head>\u00a0and the metatags (for SEO improvements) code splitting (here is\u00a0an article\u00a0solving the problem) manage the state of our app or use Redux (check this\u00a0great article  and performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website. It is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations. Seriously\u2026 Next.js can bring you all these features! 2) Next.js helps you building server rendered React.js Application  What is Next.js? Next.js is a minimalistic framework for server-rendered React applications with:  a very simple page based routing Webpack hot reloading automatic transpilation (with babel) deployment facilities automatic code splitting (loads page faster) built in css support ability to run server-side actions simple integration with Redux using next-redux-wrapper.  Get started in 1 minute In this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js. First, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this: \"scripts\": {    \"dev\": \"next\",    \"build\": \"next build\",    \"start\": \"next start\"  }   Create a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function): const Index = ({ title = 'Hello from Next.js' }) => (    <div>      <h1>{title}</h1>      <p className=\"App-intro\">        Is my application rendered by server or client?      </p>      <h2><code>{isClientOrServer()}</code></h2>    </div>  );    export default Index;   No need to import any library at the top of our index.js file, Next.js already knows that we are using React. Now enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!  Repeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file. You\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository. Use create-next-app You want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app: npm install -g create-next-app    create-next-app my-app  cd my-app/  npm run dev   This is all you need to do to create a React app with server-side rendering thanks to Next.js. Finally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in. Vue.js and Nuxt You\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:  vue init nuxt-community/starter-template <project-name>  What\u2019s Next?  Hope you liked this article which was mainly written in order to introduce server-side rendering with Next. If you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Baptiste Jan               Web Developer @Theodo. I like Vue.js and all the ecosystem growing around.                                   "},
{"article": "            Why? Adding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud. Setup Symfony and AWS First you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets. Amazon AWS Creating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket. Follow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents. Symfony Now you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:  VichUploader (a bundle that will ease files upload) KNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored) AWS-SDK (A SDK provided by Amazon to communicate with AWS API)  Install the two bundles and the SDK with composer:   composer require vich/uploader-bundle  composer require aws/aws-sdk-php  composer require knplabs/knp-gaufrette-bundle     Then register the bundles in AppKernel.php   public function registerBundles()      {       return [               new Vich\\UploaderBundle\\VichUploaderBundle(),               new Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),              ];      }     Bucket parameters It is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment. You will need to define four parameters to get access to your AWS bucket:  AWS_BUCKET_NAME AWS_BASE_URL AWS_KEY (only for and private buckets) AWS_SECRET_KEY (only for and private buckets)  You can find the values of these parameters in your AWS console. Configuration You will have to define a service extending Amazon AWS client and using your AWS credentials. Add this service in services.yml:   ct_file_store.s3:          class: Aws\\S3\\S3Client          factory: [Aws\\S3\\S3Client, 'factory']          arguments:              -                  version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)                  region: YOUR_AWS_S3_REGION                  credentials:                      key: '%env(AWS_KEY)%'                      secret: '%env(AWS_SECRET)%'     Now you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables. Here is a basic example:   knp_gaufrette:      stream_wrapper: ~      adapters:          document_adapter:              aws_s3:                  service_id: ct_file_store.s3                  bucket_name: '%env(AWS_BUCKET)%'                  detect_content_type: true                  options:                      create: true                      directory: document      filesystems:          document_fs:              adapter:    document_adapter    vich_uploader:      db_driver: orm      storage: gaufrette      mappings:          document:              inject_on_load: true              uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET)%/document\"              upload_destination: document_fs              delete_on_update:   false              delete_on_remove:   false      Upload files First step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them). The attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().   namespace MyBundle\\Entity;    use Doctrine\\ORM\\Mapping as ORM;  use Symfony\\Component\\HttpFoundation\\File\\File;  use Vich\\UploaderBundle\\Mapping\\Annotation as Vich;    /**   * Class Document   *   * @ORM\\Table(name=\"document\")   * @ORM\\Entity()   * @Vich\\Uploadable()   */  class Document  {      /**       * @var int       *       * @ORM\\Column(type=\"integer\", name=\"id\")       * @ORM\\Id       * @ORM\\GeneratedValue(strategy=\"AUTO\")       */      private $id;        /**       * @var string       *       * @ORM\\Column(type=\"string\", length=255, nullable=true)       */      private $documentFileName;        /**       * @var File       * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")       */      private $documentFile;        /**       * @var \\DateTime       *       * @ORM\\Column(type=\"datetime\")       */      private $updatedAt;  }     Then you can add an uploaded document to any of your entities:        /**       * @var Document       *       * @ORM\\OneToOne(       *     targetEntity=\"\\MyBundle\\Entity\\Document\",       *     orphanRemoval=true,       *     cascade={\"persist\", \"remove\"},       * )       * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")       */      private $myDocument;     Create a form type to be able to upload a document:   class UploadDocumentType extends AbstractType  {      public function buildForm(FormBuilderInterface $builder, array $options)      {          add('myDocument', VichFileType::class, [                  'label'         => false,                  'required'      => false,                  'allow_delete'  => false,                  'download_link' => true,              ]);      }  ...  }     Use this form type in your controller and pass the form to the twig:   ...  $myEntity = new MyEntity();  $form = $this->createForm(UploadDocumentType::class, $myEntity);  ...  return [ 'form' => $form->createView()];     Finally, add this form field in your twig and you should see an upload field in your form:   <div class=\"row\">      <div class=\"col-xs-4\">          {{ form_label(form.myDocument) }}      </div>      <div class=\"col-xs-8\">          {{ form_widget(form.myDocument) }}      </div>      <div class=\"col-xs-8 col-xs-offset-4\">          {{ form_errors(form.myDocument) }}      </div>  </div>     Navigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket. Users are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application. Display or download documents stored in private buckets In most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users. Get your document from private bucket You will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller. s3PrivateClient is the service we defined previously extending AWS S3 client. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.        /**       * @param string $documentName       *       * @return \\Aws\\Result|bool       */      public function getDocumentFromPrivateBucket($documentName)      {          try {              return $this->s3PrivateClient->getObject(                  [                      'Bucket' => $this->privateBucketName,                      'Key'    => 'my-documents/'.$documentName,                  ]              );          } catch (S3Exception $e) {              // Handle your exception here          }      }     Define an action with a custom route: You will need to use the method previously defined to download the file from AWS and expose it on a custom route.        /**       * @param Document $document       * @Route(\"/{id}/download-document\", name=\"download_document\")       * @return RedirectResponse|Response       */      public function downloadDocumentAction(Document $document)      {          $awsS3Uploader  = $this->get('app.service.s3_uploader');            $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());            if ($result) {              // Display the object in the browser              header(\"Content-Type: {$result['ContentType']}\");              echo $result['Body'];                return new Response();          }            return new Response('', 404);      }     Download document Eventually add a download button to access a document stored in a private bucket directly in your Symfony application.   <a href=\"{{ path('/download-document', {'id': document.id}) }}\"                      target=\"_blank\">     <i class=\"fa fa-print\">     {{ 'label_document_download'|trans }}  </a>     Public assets You may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.   <img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />                 You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Alan Rauzier               Developer at Theodo                                   "},
{"article": "            For one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products. As a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders). The impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project. Thus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems. And it worked! What is make Make is a build automation tool created in 1976, designed to solve dependency problems of the build process. It was originally used to get compiled files from source code, for instance for C language. In website development, an equivalent could be the installation of a project and its dependencies from the source code. Let me introduce a few concepts about make that we will need after. Make reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite. The architecture of the Makefile is the following:   target: [prerequisite]      command1      [command2]   And you run make target in your terminal to execute the target\u00a0commands. Simple, right? Use it for project installation What is mainly used to help project installation is a README describing the several commands you need to run to get your project installed. What if all these commands were executed by running make install? You would have your project working with one command, and no documentation to maintain anymore. I will only describe a simple way to build dependencies from your composer.json file   vendor: composer.json      composer install   This snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory. Note that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile. There is much more you can do, and I won\u2019t talk about it here. But if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018. Use it for the commonly used commands you need on your project After the project installation, a complexity for the developer is to find the commands needed to develop features locally. We decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis. What are the advantages of this:  The commands are committed and versioned All developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something It is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026 It\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options You can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!  Auto-generate a help target But after a long time, we started having a lot of commands. It was painful to find the one you wanted in the file, and even to know the one that existed So we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile. The initial snippet we used is:   .DEFAULT_GOAL := help  help:      @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'   If you add the following target and comments in your Makefile:   ## Example section  example_target: ## Description for example target          @does something   It would give this help message:  This generic, reusable snippet has another advantage: the documentation it generates is always up to date! And you can customize it to your need, for instance to display options associated to your commands.             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Martin Guillier                                                  "},
{"article": "            At some point during the development of your React Native application, you will use a Modal. A Modal is a component that appears on top of everything. There are a lot of cool libraries out there for modals, so today, we\u2019ll have a look a the best libraries for different use cases. Click on \u201cTap to play\u201d on the playground below to start:   You can experience the app on your phone here and check the code on github. Before choosing a library, you have to\u00a0answer those\u00a02 questions:  What do I want to display in the modal ? How great do I want the UX to be ?  To answer the 2nd question, we list a few criteria that make a good UX : 1\ufe0f\u20e3 The user can click on a button to close the modal 2\ufe0f\u20e3 The user can touch the background to close the modal 3\ufe0f\u20e3 The user can swipe the modal to close it 4\ufe0f\u20e3 The user can scroll inside the modal I)\u00a0Alert First, if you\u00a0simply want to display some information and perform an action based on\u00a0the decision of your user, you should probably go with a\u00a0native Alert. An alert is enough and provides a much simpler and more expected UX. You can see how it will look like below.  II) Native modal If you want to show more information to your user, like a picture or a customised button, you need a Modal. The simplest modal is the React Native modal. It gives you the bare properties to show and close the modal 1\ufe0f\u20e3, so it is really easy to use \u2705. The downside is that it requires some effort to customise so as to improve the user experience \u274c.   import { Modal } from \"react-native\";  ...          <Modal            animationType=\"slide\"            transparent={true}            visible={this.state.modalVisible}            onRequestClose={this.closeModal} // Used to handle the Android Back Button          >   III) Swipeable Modal If you want to improve the UX, you can allow the user to swipe the modal away. For example, if the modal comes from the top like a notification, it feels natural to close it by pulling it up \u2b06\ufe0f. If it comes from the bottom, the user will be surprised if they cannot swipe it down \u2b07\ufe0f. It\u2019s even better to highlight the fact that they can swipe the modal with a little bar with some borderRadius. The best library for that use case would be the react-native-modal library. It is widely customisable and answers to criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 3\ufe0f\u20e3.   import Modal from \"react-native-modal\";  ...          <Modal            isVisible={this.state.visible}            backdropOpacity={0.1}            swipeDirection=\"left\"            onSwipe={this.closeModal}            onBackdropPress={this.closeModal}          >   IV) Scrollable modal So far so good, now let\u2019s see some more complex use cases. For instance, you may want the content of the modal to be scrollable (if you are displaying a lot of content or a Flatlist). The scroll may conflict with either the scroll of the modal or the scroll of the container of the Modal, if it is a scrollable component. For this use case, you can still use the react-native-modal library. You will have 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 4\ufe0f\u20e3. You can control the direction of the swipe with\u2026 swipeDirection.   import Modal from \"react-native-modal\";  ...          <Modal            isVisible={this.state.visible}            backdropOpacity={0.1}            onSwipe={this.closeModal}            // swipeDirection={\"left\"} <-- We can't specify swipeDirection since we want to scroll inside the modal            onBackdropPress={this.closeModal}          >   \u26a0\ufe0f Don\u2019t try to combine swipeable + scrollable with this library. Instead continue reading\u2026 V) Swipeable + Scrollable modal The previous libraries are already awesome, but if you want your modal to answer criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3, 3\ufe0f\u20e3and 4\ufe0f\u20e3, you need\u00a0react-native-modalbox. This library is still very easy to use \u2705and has everything out of the box \u2705, and is listed in the awesome libraries by awesome-react-native. The only downside is that the modal from this library always appear from the bottom, and you can only swipe it down \u274c.   import Modal from \"react-native-modalbox\";  ...          <Modal            style={styles.container}            swipeToClose={true}            swipeArea={20} // The height in pixels of the swipeable area, window height by default            swipeThreshold={50} // The threshold to reach in pixels to close the modal            isOpen={this.state.isOpen}            onClosed={this.closeModal}            backdropOpacity={0.1}          >   To avoid the collision between the scroll of your content and the swipe to close the modal, you have to specify swipeArea and swipeThreshold. Conclusion There are a lot of libraries built on top of the native modal. It is important to choose the right one depending on your needs. If you want to control the direction of the swipe, use react-native-modal, but if you want the modal to only come from the bottom, use react-native-modalbox. The libraries I\u2019ve talked about are amazing. Thanks to their contributors.  Please reach out if you think I missed something.             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us               WRITTEN BY                                                           Antoine Garcia                                                  "},
{"article": "            tl:dr To add a pre-commit git hook with Husky:  Install Husky with npm install husky --save-dev Set the pre-commit command in your package.json:  \"scripts\": {      \"precommit\": \"npm test\"  },   What are git hooks? Git hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created. The scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.  Why do I need to install the Husky\u00a0package then? The problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers. Husky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:  Theses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command \"scripts\": {      \"precommit\": \"npm test\"  },   To add husky to your project, simply run npm install husky --save-dev. For more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\". Enhancing your git flow Git hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.  Check for linting errors  If you have tools to check the code quality or formatting, you can run it on a pre-commit hook: \"scripts\": {      \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"  },   I advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.  Protect important branches  In some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook: \"scripts\": {      \"prepush\": \"./scripts/pre-push-check.sh\"  },    #!/bin/bash  set -e    branch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')  if [ \"$branch\" == \"master\" ]  then      npm test  fi    If your tests fail, the code won\u2019t be pushed.             You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us                 WRITTEN BY                                                           Hugo Lime               Agile Web Developer at Theodo.    Passionate about new technologies to make web apps stronger and faster.                                   "}
]